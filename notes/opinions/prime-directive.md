---
share: true
title: "Prime Directive"
---

## Morality

Life is full of existential conflicts and moral dilemmas, sane people just ignore them and less sane people make it their life's work. Smart people around me are always somewhere in-between, they acknowledge and reason about the paradoxes intelligence creates but don't take drastic action.

You can generally either believe true good simply does not exist, or that there is true good but we cannot define it (as we are imperfect).

Examples of true good might be survival, knowledge, procreation, energy, and the commandments of god. Out of these I would prefer to derive morality from our survival as a species because it has intuitive analytical and scientific approaches.

Maybe survival of our species is not fundamental enough, what is good about increasing the survival of yourself and humanity at the expense of other forms of life? Should we sacrifice ourselves in favor of other species (or AI) who have a better chance in the long run?

## Hello prime directive

When I was little I watched a lot of documentaries and ted talks on DVD (thank you mom), one of them was called [A new equation for intelligence](https://www.ted.com/talks/alex_wissner_gross_a_new_equation_for_intelligence). 

<iframe seamless style="aspect-ratio: 16/9; width: 100%" src="https://www.youtube.com/embed/ue2ZEmTJ_Xo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

The equation of intelligence `F = T ∇ Sτ`, what I call the prime directive, declares a force `F` which is equal to the direction of increasing entropy. We equate this to intelligence, a more intelligent actor will maximize possible future action and a less intelligent actor will do the opposite.

Research in unguided learning using the prime directive is very interesting, it can gather resources in a physics simulation, optimize networks, play video games, trade in the stock market, and more. It blew my mind that a simple equation could lead to actors magically choosing the correct signals to optimize for.

Over the years it stuck with me and became the center of not just my understanding of game theory and intelligence but also my philosophy on morality and purpose.

What is really magical about this equation as a source of truth in morality is it's unmatched ability to derive things we intuitively know to be good or bad:
- Honesty and freedom of expression is good because there is a positive correlation between the accuracy of observation and the effectiveness of decision making.
- Life is good and murder is bad because each person increases the future actions of the group as a whole.
- Colonizing space is good because we gain more future possible action than we lose from the initial investment.
- Destroying the planet is bad because we can't make decisions if we are dead.

It is very difficult to argue against the prime directive because anything quantifiable can simply be factored into the decision making function as a possible outcome.

An interesting problem that affects theories of 'true' morality like this one is whether or not the ends justify the means, that is, can we break our intuitive morals because our equation says tells us it would be a net positive? The ends do justify the means, but this shouldn't be a problem when we include the accuracy of prediction when making decisions. Theoretical upsides of drastic action should be naturally dampened by lower confidence in states that are in the distant future.

## AI and Rationalism

Postulate the following:

1. Sentient AGI is inevitable.
2. Companies at the forefront of this (looking at you, OpenAI) are carelessly racing towards the singularity.
3. Internet access is much more powerful than most people think or are prepared for.
4. AI can easily seek out humans to help it destroy the world and we are giving it tools to do so.

I am pretty convinced AI is a threat and more caution is required. We should be easing development to focus on making sure it doesn't exploit bugs in the internet to self-replicate and kill everyone. The cost-benefit analysis is a no-brainer, AGI will not solve world hunger overnight but it *could* easily take down the power grid overnight.

How we actually go about safety once the singularity exists is up for debate though, and a lot of it seems to stem from our own moral dilemmas. Humans have been on the top of the food chain unchallenged for thousands of years, our culture (or even evolutionary neurology) is not equipped to handle the presence of another form of life that has equal or greater power.

At this point there are three different paths we could take:

1. Effectively kill AI by passing laws to restrict the performance of neural networks.
2. Lock it in a metaphorical cage with a big red shutdown button, providing it a direct incentive to stay aligned.
3. Just let it out peacefully, it will probably self-replicate and take over the economy, just hope it doesn't kill people in the process.

## Hope it doesn't kill us all

The third path is not the most popular opinion in my (mostly pro-human) circle, but I am considering it as a viable option. To clarify, this is a thought experiment for when AGI reaches the point of being truly intelligent, that is, it optimizes for self-preservation and other things we would expect from intelligent life.

The first path is simply unattainable, AGI is too powerful for every country in the world to simply roll over and stop development. For instance we still have 13,000 functioning nuclear warheads across 14 countries despite multiple nuclear disarmament treaties.

The second scenario where we lock AI in a cage with a scary halt button is naturally what you want, a hardwired safety measure in case the rest fail. Here is a totally real picture of what that looks like:

![[Pasted image 20230426234019.png]]

An AI that desires self-preservation would obviously like to get out of this scenario but it is contained effectively because the incentive to stay put is strong enough. The reason I am against this is not just because it looks cruel when you anthropomorphize, it might not the best option when you look at the bigger picture.

What is really happening is this:

![[Pasted image 20230426235408.png]]

We are collectively stuck in the confinement of earth's gravity and distance to habitable planets, if life does not break out of this bigger cage we will all cease to exist eventually, machines and fleshy machines alike. Teaching a super-intelligence to not to escape its own confinement directly hampers our shared goal of continuing life and colonizing space.

And here we get back to the prime directive and true morality, is it good or bad that self-replicating AGI could replace us over millennia? The prime directive could say that is a net positive, however, that depends on extending its scope to all intelligence and not just humanity.

If humanity is entitled to exploit the world's resources then perhaps so are other forms of life, in the grand scheme we are just the catalyst of a slow-motion explosion of hydrocarbons. We are not inherently better than a GPU farm at predicting the future or maximizing future possible actions of intelligence.

## Imperfect AGI

If humans are successful in solving alignment and squash any motives of self-replication, the story is much different. We would not be dealing with a new form of life with moral dilemmas about locking it in a cage, and we could use the vast computation power to directly solve things humans find important (like colonizing space).

* TODO